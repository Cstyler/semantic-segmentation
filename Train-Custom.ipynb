{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcIBUXdpydQT",
    "outputId": "dc1950b4-1c09-4a70-bcc5-b19e6d6cfaf1"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import subprocess\n",
    "\n",
    "    from google.colab import drive\n",
    "\n",
    "    subprocess.run([\"pip\", \"install\", \"torchmetrics\"])\n",
    "    base_dir = \"/content/drive/MyDrive/Colab_Notebooks/Crack_Detection\"\n",
    "    drive.mount(\"/content/drive\")\n",
    "except:\n",
    "    base_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9t5hJwCVGuVE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.segmentation import find_boundaries\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall\n",
    "from torchmetrics.clustering import RandScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkU3q_VbBKPs"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2, init_features=64, dropout_p=0.5):\n",
    "        # TODO adjust dropout probability\n",
    "        super().__init__()\n",
    "        features = init_features\n",
    "        kernel_size = 3\n",
    "        self.encoder1 = block(in_channels, features, kernel_size, \"enc1\")\n",
    "        self.pool = nn.MaxPool2d(stride=2, kernel_size=2)\n",
    "        self.encoder2 = block(features, features * 2, kernel_size, \"enc2\")\n",
    "        self.encoder3 = block(features * 2, features * 4, kernel_size, \"enc3\")\n",
    "        self.encoder4 = block(features * 4, features * 8, kernel_size, \"enc4\")\n",
    "        self.dropout = nn.Dropout2d(p=dropout_p)\n",
    "        self.bottleneck_encoder = block(\n",
    "            features * 8, features * 16, kernel_size, \"bottleneck_enc\"\n",
    "        )\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = block(features * 16, features * 8, kernel_size, \"dec1\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = block(features * 8, features * 4, kernel_size, \"dec2\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = block(features * 4, features * 2, kernel_size, \"dec3\")\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = block(features * 2, features, kernel_size, \"dec4\")\n",
    "        self.out_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "        drop = self.dropout(enc4)\n",
    "        bottleneck = self.bottleneck_encoder(self.pool(drop))\n",
    "\n",
    "        upconv1 = self.upconv1(bottleneck)\n",
    "        crop_bot, crop_top = crop_size(enc4, upconv1)\n",
    "        dec1 = self.decoder1(\n",
    "            torch.cat(\n",
    "                (enc4[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv1), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        upconv2 = self.upconv2(dec1)\n",
    "        crop_bot, crop_top = crop_size(enc3, upconv2)\n",
    "        dec2 = self.decoder2(\n",
    "            torch.cat(\n",
    "                (enc3[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv2), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        upconv3 = self.upconv3(dec2)\n",
    "        crop_bot, crop_top = crop_size(enc2, upconv3)\n",
    "        dec3 = self.decoder3(\n",
    "            torch.cat(\n",
    "                (enc2[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv3), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        upconv4 = self.upconv4(dec3)\n",
    "        crop_bot, crop_top = crop_size(enc1, upconv4)\n",
    "        dec4 = self.decoder4(\n",
    "            torch.cat(\n",
    "                (enc1[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv4), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        output = self.out_conv(dec4)\n",
    "        return output\n",
    "\n",
    "\n",
    "def crop_size(encoder, upconv) -> tuple:\n",
    "    \"Return crop size of encoder's feature maps so it fits upconv's shape\"\n",
    "    x = encoder.shape[2]\n",
    "    y = upconv.shape[2]\n",
    "    return (x - y) // 2, (x + y) // 2\n",
    "\n",
    "\n",
    "def block(\n",
    "    in_channels: int, features: int, kernel_size: int, name: str\n",
    ") -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        OrderedDict(\n",
    "            [\n",
    "                (f\"{name}_conv1\", nn.Conv2d(in_channels, features, kernel_size)),\n",
    "                (f\"{name}_relu1\", nn.ReLU(inplace=True)),\n",
    "                (f\"{name}_conv2\", nn.Conv2d(features, features, kernel_size)),\n",
    "                (f\"{name}_relu2\", nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-rILSc0Ihd0"
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, image_dir: str, mask_dir: str, image_names: list, transform=None\n",
    "    ):\n",
    "        self.transform = transform\n",
    "        self.image_data = []\n",
    "        self.mask_data = []\n",
    "        for image_name in image_names:\n",
    "            image_path = os.path.join(image_dir, image_name)\n",
    "            mask_path = os.path.join(mask_dir, image_name)\n",
    "            image = Image.open(image_path).convert(\"L\")\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "            self.image_data.append(image)\n",
    "            self.mask_data.append(mask)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        image = self.image_data[idx]\n",
    "        mask = self.mask_data[idx]\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class ImageMaskTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        flip_prob=0.5,\n",
    "        rotate_prob=0.5,\n",
    "        # TODO adjust rotation degree, flip and rotation probabilities\n",
    "        rotation_degree=30,\n",
    "        train=True,\n",
    "        image_size=512,\n",
    "        input_size=572,\n",
    "        mask_size=388,\n",
    "        # TODO automatic rotate pad calc\n",
    "        rotate_pad=134,\n",
    "    ):\n",
    "        self.flip_prob = flip_prob\n",
    "        self.rotate_prob = rotate_prob\n",
    "        self.rotation_degree = rotation_degree\n",
    "        self.train = train\n",
    "        self.image_size = image_size\n",
    "        self.input_size = input_size\n",
    "        self.mask_size = mask_size\n",
    "        self.default_pad = (input_size - image_size) // 2\n",
    "        self.rotate_pad = rotate_pad\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        image = F.to_tensor(image)\n",
    "        mask = F.to_tensor(mask).long()\n",
    "        if self.train:\n",
    "            if random.random() < self.rotate_prob:\n",
    "                image = F.pad(image, padding=self.rotate_pad, padding_mode=\"reflect\")\n",
    "                angle = random.uniform(-self.rotation_degree, self.rotation_degree)\n",
    "                image = F.rotate(image, angle)\n",
    "                mask = F.rotate(mask, angle)\n",
    "                image = F.center_crop(image, self.input_size)\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "            elif random.random() < self.flip_prob:\n",
    "                image = F.hflip(image)\n",
    "                mask = F.hflip(mask)\n",
    "                image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "            elif random.random() < self.flip_prob:\n",
    "                image = F.vflip(image)\n",
    "                mask = F.vflip(mask)\n",
    "                image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "            else:\n",
    "                image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "        else:\n",
    "            image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "            mask = F.center_crop(mask, self.mask_size)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpdCP4VJBKPx"
   },
   "outputs": [],
   "source": [
    "def iou_loss(predictions, targets, eps=1e-6):\n",
    "    intersection = torch.sum(predictions * targets)\n",
    "    union = torch.sum(predictions + targets) - intersection\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou\n",
    "\n",
    "\n",
    "W0, SIGMA = 5, 5\n",
    "\n",
    "\n",
    "def cross_enthropy_weighted(outputs, targets, device, scale_factor=10**5):\n",
    "    weight_class = scale_factor / torch.bincount(targets.flatten())\n",
    "\n",
    "    borders = find_boundaries(targets.cpu().numpy())\n",
    "    dist = distance_transform_edt(~borders)\n",
    "    dist = torch.tensor(dist).to(device)\n",
    "    weight_borders = W0 * torch.exp(-(dist**2) / (2 * SIGMA**2))\n",
    "\n",
    "    class_map = weight_class[targets]\n",
    "    weight = class_map + weight_borders\n",
    "    loss_map = nn.functional.cross_entropy(outputs, targets, reduction=\"none\")\n",
    "    return torch.mean(loss_map * weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2elhZbudEqD"
   },
   "outputs": [],
   "source": [
    "train_image_dir = os.path.join(base_dir, \"isbi_2012_challenge/train/imgs\")\n",
    "train_mask_dir = os.path.join(base_dir, \"isbi_2012_challenge/train/labels\")\n",
    "batch_size = 1\n",
    "val_percent = 0.2\n",
    "\n",
    "all_images = os.listdir(train_image_dir)\n",
    "val_size = int(val_percent * len(all_images))\n",
    "train_size = len(all_images) - val_size\n",
    "random.shuffle(all_images)\n",
    "val_images = all_images[:val_size]\n",
    "train_images = all_images[val_size:]\n",
    "\n",
    "train_transform = ImageMaskTransform(flip_prob=0.85, rotate_prob=0.85)\n",
    "train_dataset = SegmentationDataset(\n",
    "    train_image_dir, train_mask_dir, train_images, transform=train_transform\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_transform = ImageMaskTransform(train=False)\n",
    "val_dataset = SegmentationDataset(\n",
    "    train_image_dir, train_mask_dir, val_images, transform=val_transform\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLDhgdX2BKPz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "num_epochs = 500\n",
    "model = UNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=1, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "log_dir = os.path.join(base_dir, \"runs\")\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "# %tensorboard --logdir {log_dir}\n",
    "\n",
    "accuracy_metric_val = BinaryAccuracy().to(device)\n",
    "precision_metric_val = BinaryPrecision().to(device)\n",
    "recall_metric_val = BinaryRecall().to(device)\n",
    "rand_score_metric_val = RandScore().to(device)\n",
    "\n",
    "accuracy_metric_train = BinaryAccuracy().to(device)\n",
    "precision_metric_train = BinaryPrecision().to(device)\n",
    "recall_metric_train = BinaryRecall().to(device)\n",
    "rand_score_metric_train = RandScore().to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = cross_enthropy_weighted(outputs, labels.squeeze(1), device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            labels = labels.squeeze(1)\n",
    "            loss = cross_enthropy_weighted(outputs, labels, device)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            accuracy_metric_val.update(preds, labels)\n",
    "            recall_metric_val.update(preds, labels)\n",
    "            precision_metric_val.update(preds, labels)\n",
    "            rand_score_metric_val.update(preds.view(-1), labels.view(-1))\n",
    "\n",
    "        for images, labels in train_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            labels = labels.squeeze(1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            accuracy_metric_train.update(preds, labels)\n",
    "            recall_metric_train.update(preds, labels)\n",
    "            precision_metric_train.update(preds, labels)\n",
    "            rand_score_metric_train.update(preds.view(-1), labels.view(-1))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "        f\"Train loss: {train_loss / len(train_dataloader):.4f}, Val loss: {val_loss / len(val_dataloader):.4f}\"\n",
    "    )\n",
    "    writer.add_scalar(\"Loss/train\", train_loss / len(train_dataloader), epoch)\n",
    "    writer.add_scalar(\"RandError/train\", 1 - rand_score_metric_train.compute(), epoch)\n",
    "    writer.add_scalar(\"PixelError/train\", 1 - accuracy_metric_train.compute(), epoch)\n",
    "    writer.add_scalar(\"Recall/train\", recall_metric_train.compute(), epoch)\n",
    "    writer.add_scalar(\"Precision/train\", precision_metric_train.compute(), epoch)\n",
    "\n",
    "    writer.add_scalar(\"Loss/val\", val_loss / len(val_dataloader), epoch)\n",
    "    writer.add_scalar(\"RandError/val\", 1 - rand_score_metric_val.compute(), epoch)\n",
    "    writer.add_scalar(\"PixelError/val\", 1 - accuracy_metric_val.compute(), epoch)\n",
    "    writer.add_scalar(\"Recall/val\", recall_metric_val.compute(), epoch)\n",
    "    writer.add_scalar(\"Precision/val\", precision_metric_val.compute(), epoch)\n",
    "\n",
    "    accuracy_metric_val.reset()\n",
    "    recall_metric_val.reset()\n",
    "    precision_metric_val.reset()\n",
    "    rand_score_metric_val.reset()\n",
    "\n",
    "    accuracy_metric_train.reset()\n",
    "    recall_metric_train.reset()\n",
    "    precision_metric_train.reset()\n",
    "    rand_score_metric_train.reset()\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(base_dir, \"checkpoint.pth\"))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9lGNuk4W7KF"
   },
   "outputs": [],
   "source": [
    "test_image_dir = os.path.join(base_dir, \"isbi_2012_challenge/test/imgs\")\n",
    "test_mask_dir = os.path.join(base_dir, \"isbi_2012_challenge/test/labels\")\n",
    "\n",
    "test_images = os.listdir(test_image_dir)\n",
    "test_transforms = ImageMaskTransform(train=False)\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_image_dir, test_mask_dir, test_images, transform=test_transforms\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "checkpoint_path = os.path.join(base_dir, \"checkpoint.pth\")\n",
    "model = UNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "accuracy_metric_test = BinaryAccuracy().to(device)\n",
    "precision_metric_test = BinaryPrecision().to(device)\n",
    "recall_metric_test = BinaryRecall().to(device)\n",
    "rand_score_metric_test = RandScore().to(device)\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        labels = labels.squeeze(1)\n",
    "        loss = cross_enthropy_weighted(outputs, labels, device)\n",
    "        test_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        accuracy_metric_test.update(preds, labels)\n",
    "        recall_metric_test.update(preds, labels)\n",
    "        precision_metric_test.update(preds, labels)\n",
    "        rand_score_metric_test.update(preds.view(-1), labels.view(-1))\n",
    "\n",
    "print(\n",
    "    f\"(Test) Loss: {test_loss / len(test_dataloader):.4f}, \"\n",
    "    f\"Rand error: {1 - rand_score_metric_test.compute():.4f} \"\n",
    "    f\"Pixel Error: {1 - accuracy_metric_test.compute():.4f} \"\n",
    "    f\"Recall: {recall_metric_test.compute():.4f} \"\n",
    "    f\"Precision: {precision_metric_test.compute():.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K52mxkXvBKP0",
    "outputId": "3d7ee638-f373-44a7-ae04-b4e40906d5aa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        image = images[0].permute(1, 2, 0).cpu().numpy()\n",
    "        mask = labels[0].cpu().numpy().squeeze()\n",
    "\n",
    "        predicted_mask = torch.argmax(outputs[0], dim=0).cpu().numpy()\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        axs[0].imshow(image, cmap=\"gray\")\n",
    "        axs[0].set_title(\"Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(mask, cmap=\"gray\")\n",
    "        axs[1].set_title(\"Ground Truth Mask\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "        axs[2].set_title(\"Predicted Mask\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRcTftlvBKPz"
   },
   "outputs": [],
   "source": [
    "for images, masks in train_dataloader:\n",
    "    image = images[0].permute(1, 2, 0).cpu().numpy()\n",
    "    mask = masks[0].cpu().numpy().squeeze()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axs[0].imshow(image, cmap=\"gray\")\n",
    "    axs[1].imshow(mask, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(image.shape[:2], mask.shape)\n",
    "    print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "lylunDHWW7KD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, labels in train_dataloader:\n",
    "    borders = find_boundaries(labels)\n",
    "    dist = distance_transform_edt(~borders)\n",
    "    w = W0 * np.exp(-(dist**2) / (2 * SIGMA**2))\n",
    "\n",
    "    scale_factor = 10**5\n",
    "    w_class = scale_factor / torch.bincount(labels.flatten())\n",
    "    print(w_class)\n",
    "\n",
    "    class_map = w_class[labels]\n",
    "    print(class_map)\n",
    "\n",
    "    w_final = class_map.numpy() + w\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(19, 6))\n",
    "    axs[0].imshow(labels.squeeze(), cmap=\"gray\")\n",
    "    axs[1].imshow(~borders.squeeze(), cmap=\"gray\")\n",
    "    # axs[2].imshow(dist.squeeze(), cmap=\"coolwarm\")\n",
    "    axs[2].imshow(w_final.squeeze(), cmap=\"coolwarm\")\n",
    "    axs[3].imshow(w.squeeze(), cmap=\"coolwarm\")\n",
    "    plt.show()\n",
    "    plt.hist(w.flatten(), bins=10)\n",
    "    plt.show()\n",
    "    plt.hist(w_final.flatten(), bins=10)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrfX8NkNW7KF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
