{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcIBUXdpydQT"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "9t5hJwCVGuVE",
    "outputId": "bb375c98-2c26-44d7-8f18-d138845d535e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import socket\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall\n",
    "from torchmetrics.clustering import RandScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HkU3q_VbBKPs"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2, init_features=64, dropout_p=0.5):\n",
    "        # TODO adjust dropout probability\n",
    "        super().__init__()\n",
    "        features = init_features\n",
    "        kernel_size = 3\n",
    "        self.encoder1 = block(in_channels, features, kernel_size, \"enc1\")\n",
    "        self.pool = nn.MaxPool2d(stride=2, kernel_size=2)\n",
    "        self.encoder2 = block(features, features * 2, kernel_size, \"enc2\")\n",
    "        self.encoder3 = block(features * 2, features * 4, kernel_size, \"enc3\")\n",
    "        self.encoder4 = block(features * 4, features * 8, kernel_size, \"enc4\")\n",
    "        self.dropout = nn.Dropout2d(p=dropout_p)\n",
    "        self.bottleneck_encoder = block(\n",
    "            features * 8, features * 16, kernel_size, \"bottleneck_enc\"\n",
    "        )\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = block(features * 16, features * 8, kernel_size, \"dec1\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = block(features * 8, features * 4, kernel_size, \"dec2\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = block(features * 4, features * 2, kernel_size, \"dec3\")\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = block(features * 2, features, kernel_size, \"dec4\")\n",
    "        self.out_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "        drop = self.dropout(enc4)\n",
    "        bottleneck = self.bottleneck_encoder(self.pool(drop))\n",
    "\n",
    "        upconv1 = self.upconv1(bottleneck)\n",
    "        crop_bot, crop_top = crop_size(enc4, upconv1)\n",
    "        dec1 = self.decoder1(\n",
    "            torch.cat(\n",
    "                (enc4[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv1), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        upconv2 = self.upconv2(dec1)\n",
    "        crop_bot, crop_top = crop_size(enc3, upconv2)\n",
    "        dec2 = self.decoder2(\n",
    "            torch.cat(\n",
    "                (enc3[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv2), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        upconv3 = self.upconv3(dec2)\n",
    "        crop_bot, crop_top = crop_size(enc2, upconv3)\n",
    "        dec3 = self.decoder3(\n",
    "            torch.cat(\n",
    "                (enc2[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv3), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        upconv4 = self.upconv4(dec3)\n",
    "        crop_bot, crop_top = crop_size(enc1, upconv4)\n",
    "        dec4 = self.decoder4(\n",
    "            torch.cat(\n",
    "                (enc1[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv4), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        output = self.out_conv(dec4)\n",
    "        return output\n",
    "\n",
    "\n",
    "def crop_size(encoder, upconv) -> tuple:\n",
    "    \"Return crop size of encoder's feature maps so it fits upconv's shape\"\n",
    "    x = encoder.shape[2]\n",
    "    y = upconv.shape[2]\n",
    "    return (x - y) // 2, (x + y) // 2\n",
    "\n",
    "\n",
    "def block(\n",
    "    in_channels: int, features: int, kernel_size: int, name: str\n",
    ") -> nn.Sequential:\n",
    "    # TODO add BatchNorm2d?\n",
    "    return nn.Sequential(\n",
    "        OrderedDict(\n",
    "            [\n",
    "                (f\"{name}_conv1\", nn.Conv2d(in_channels, features, kernel_size)),\n",
    "                (f\"{name}_relu1\", nn.ReLU(inplace=True)),\n",
    "                (f\"{name}_conv2\", nn.Conv2d(features, features, kernel_size)),\n",
    "                (f\"{name}_relu2\", nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y-rILSc0Ihd0"
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir: str, mask_dir: str, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        # self.images = random.sample(os.listdir(image_dir), 1095)\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.image_data = []\n",
    "        self.mask_data = []\n",
    "        for image_name in self.images:\n",
    "            image_path = os.path.join(self.image_dir, image_name)\n",
    "            mask_path = os.path.join(self.mask_dir, image_name)\n",
    "            image = Image.open(image_path).convert(\"L\")\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "            self.image_data.append(image)\n",
    "            self.mask_data.append(mask)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        image = self.image_data[idx]\n",
    "        mask = self.mask_data[idx]\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class ImageMaskTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        flip_prob=0.85,\n",
    "        rotate_prob=0.85,\n",
    "        # TODO adjust rotation degree, flip and rotation probabilities\n",
    "        rotation_degree=30,\n",
    "        train=True,\n",
    "        image_size=512,\n",
    "        input_size=572,\n",
    "        mask_size=388,\n",
    "        # TODO automatic rotate pad calc\n",
    "        rotate_pad=134,\n",
    "    ):\n",
    "        self.flip_prob = flip_prob\n",
    "        self.rotate_prob = rotate_prob\n",
    "        self.rotation_degree = rotation_degree\n",
    "        self.train = train\n",
    "        self.image_size = image_size\n",
    "        self.input_size = input_size\n",
    "        self.mask_size = mask_size\n",
    "        self.default_pad = (input_size - image_size) // 2\n",
    "        self.rotate_pad = rotate_pad\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        image = F.to_tensor(image)\n",
    "        mask = F.to_tensor(mask).long()\n",
    "        if self.train:\n",
    "            if random.random() < self.rotate_prob:\n",
    "                image = F.pad(image, padding=self.rotate_pad, padding_mode=\"reflect\")\n",
    "                angle = random.uniform(-self.rotation_degree, self.rotation_degree)\n",
    "                image = F.rotate(image, angle)\n",
    "                mask = F.rotate(mask, angle)\n",
    "                image = F.center_crop(image, self.input_size)\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "            elif random.random() < self.flip_prob:\n",
    "                image = F.hflip(image)\n",
    "                mask = F.hflip(mask)\n",
    "                image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "            elif random.random() < self.flip_prob:\n",
    "                image = F.vflip(image)\n",
    "                mask = F.vflip(mask)\n",
    "                image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "            else:\n",
    "                image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "        else:\n",
    "            image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "            mask = F.center_crop(mask, self.mask_size)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MpdCP4VJBKPx"
   },
   "outputs": [],
   "source": [
    "def iou_loss(predictions, targets, eps=1e-6):\n",
    "    intersection = torch.sum(predictions * targets)\n",
    "    union = torch.sum(predictions + targets) - intersection\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou\n",
    "\n",
    "\n",
    "class CrossEntropyIoULoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyIoULoss, self).__init__()\n",
    "        weight = torch.tensor([3.0, 1.0]).to(device)\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        ce_loss = self.ce(outputs, targets)\n",
    "        return ce_loss\n",
    "        # TODO add IOU\n",
    "        # return bce_loss + iou_loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrXUBgZjBokq",
    "outputId": "0ccce48f-6b05-4450-9d2d-1f8f90c5e1db"
   },
   "outputs": [],
   "source": [
    "local_machine_name = \"Dratini\"\n",
    "if socket.gethostname().endswith(local_machine_name):\n",
    "    base_dir = \"./\"\n",
    "else:\n",
    "    base_dir = \"/content/drive/MyDrive/Colab_Notebooks/Crack_Detection/\"\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M2elhZbudEqD"
   },
   "outputs": [],
   "source": [
    "train_transform = ImageMaskTransform()\n",
    "\n",
    "train_image_dir = base_dir + \"isbi_2012_challenge/train/imgs\"\n",
    "train_mask_dir = base_dir + \"isbi_2012_challenge/train/labels\"\n",
    "batch_size = 1\n",
    "train_dataset = SegmentationDataset(\n",
    "    train_image_dir, train_mask_dir, transform=train_transform\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_image_dir = base_dir + \"isbi_2012_challenge/test/imgs\"\n",
    "test_mask_dir = base_dir + \"isbi_2012_challenge/test/labels\"\n",
    "\n",
    "test_transforms = ImageMaskTransform(train=False)\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_image_dir, test_mask_dir, transform=test_transforms\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRcTftlvBKPz"
   },
   "outputs": [],
   "source": [
    "for images, masks in train_dataloader:\n",
    "    image = images[0].permute(1, 2, 0).cpu().numpy()\n",
    "    mask = masks[0].cpu().numpy().squeeze()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axs[0].imshow(image, cmap=\"gray\")\n",
    "    axs[1].imshow(mask, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(image.shape[:2], mask.shape)\n",
    "    print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLDhgdX2BKPz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "num_epochs = 100\n",
    "model = UNet()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = CrossEntropyIoULoss()\n",
    "\n",
    "accuracy_metric_test = BinaryAccuracy().to(device)\n",
    "precision_metric_test = BinaryPrecision().to(device)\n",
    "recall_metric_test = BinaryRecall().to(device)\n",
    "rand_score_metric_test = RandScore().to(device)\n",
    "\n",
    "accuracy_metric_train = BinaryAccuracy().to(device)\n",
    "precision_metric_train = BinaryPrecision().to(device)\n",
    "recall_metric_train = BinaryRecall().to(device)\n",
    "rand_score_metric_train = RandScore().to(device)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            labels = labels.squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            accuracy_metric_test.update(preds, labels)\n",
    "            recall_metric_test.update(preds, labels)\n",
    "            precision_metric_test.update(preds, labels)\n",
    "            rand_score_metric_test.update(preds.view(-1), labels.view(-1))\n",
    "\n",
    "        for images, labels in train_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            labels = labels.squeeze(1)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            accuracy_metric_train.update(preds, labels)\n",
    "            recall_metric_train.update(preds, labels)\n",
    "            precision_metric_train.update(preds, labels)\n",
    "            rand_score_metric_train.update(preds.view(-1), labels.view(-1))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "        f\"(Train) Loss: {train_loss / len(train_dataloader):.4f} \"\n",
    "        f\"Rand error: {1 - rand_score_metric_train.compute():.4f} \"\n",
    "        f\"Pixel Error: {1 - accuracy_metric_train.compute():.4f} \"\n",
    "        f\"Recall: {recall_metric_train.compute():.4f} \"\n",
    "        f\"Precision: {precision_metric_train.compute():.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"             (Test) Loss: {test_loss / len(test_dataloader):.4f}, \"\n",
    "        f\"Rand error: {1 - rand_score_metric_test.compute():.4f} \"\n",
    "        f\"Pixel Error: {1 - accuracy_metric_test.compute():.4f} \"\n",
    "        f\"Recall: {recall_metric_test.compute():.4f} \"\n",
    "        f\"Precision: {precision_metric_test.compute():.4f}\"\n",
    "    )\n",
    "    accuracy_metric_test.reset()\n",
    "    recall_metric_test.reset()\n",
    "    precision_metric_test.reset()\n",
    "    rand_score_metric_test.reset()\n",
    "\n",
    "    accuracy_metric_train.reset()\n",
    "    recall_metric_train.reset()\n",
    "    precision_metric_train.reset()\n",
    "    rand_score_metric_train.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbbzFJW_BKP1"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), base_dir + \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ale9nDeagGn-",
    "outputId": "1a5300fe-0d7b-4e50-9f5e-aad83619f8dc"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = base_dir + \"checkpoint.pth\"\n",
    "model = UNet()\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "K52mxkXvBKP0",
    "outputId": "420d2a47-0003-48f2-f488-513240adb4af",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_iter = iter(test_dataloader)\n",
    "# data_iter = iter(train_dataloader)\n",
    "model.to(device)\n",
    "for images, labels in data_iter:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "\n",
    "    image = images[0].permute(1, 2, 0).cpu().numpy()\n",
    "    mask = labels[0].cpu().numpy().squeeze()\n",
    "\n",
    "    predicted_mask = torch.argmax(outputs[0], dim=0).cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axs[0].imshow(image, cmap=\"gray\")\n",
    "    axs[0].set_title(\"Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(mask, cmap=\"gray\")\n",
    "    axs[1].set_title(\"Ground Truth Mask\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtaJDCsQBKP1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
