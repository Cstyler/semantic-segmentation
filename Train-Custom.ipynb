{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9t5hJwCVGuVE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import socket\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkU3q_VbBKPs"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=2, init_features=64, dropout_p=0.5):\n",
    "        # TODO adjust dropout probability\n",
    "        super().__init__()\n",
    "        features = init_features\n",
    "        kernel_size = 3\n",
    "        self.encoder1 = block(in_channels, features, kernel_size, \"enc1\")\n",
    "        self.pool = nn.MaxPool2d(stride=2, kernel_size=2)\n",
    "        self.encoder2 = block(features, features * 2, kernel_size, \"enc2\")\n",
    "        self.encoder3 = block(features * 2, features * 4, kernel_size, \"enc3\")\n",
    "        self.encoder4 = block(features * 4, features * 8, kernel_size, \"enc4\")\n",
    "        self.dropout = nn.Dropout2d(p=dropout_p)\n",
    "        self.bottleneck_encoder = block(\n",
    "            features * 8, features * 16, kernel_size, \"bottleneck_enc\"\n",
    "        )\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = block(features * 16, features * 8, kernel_size, \"dec1\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = block(features * 8, features * 4, kernel_size, \"dec2\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = block(features * 4, features * 2, kernel_size, \"dec3\")\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = block(features * 2, features, kernel_size, \"dec4\")\n",
    "        self.out_conv = nn.Conv2d(features, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "        drop = self.dropout(enc4)\n",
    "        bottleneck = self.bottleneck_encoder(self.pool(drop))\n",
    "\n",
    "        upconv1 = self.upconv1(bottleneck)\n",
    "        crop_bot, crop_top = crop_size(enc4, upconv1)\n",
    "        dec1 = self.decoder1(\n",
    "            torch.cat(\n",
    "                (enc4[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv1), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        upconv2 = self.upconv2(dec1)\n",
    "        crop_bot, crop_top = crop_size(enc3, upconv2)\n",
    "        dec2 = self.decoder2(\n",
    "            torch.cat(\n",
    "                (enc3[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv2), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        upconv3 = self.upconv3(dec2)\n",
    "        crop_bot, crop_top = crop_size(enc2, upconv3)\n",
    "        dec3 = self.decoder3(\n",
    "            torch.cat(\n",
    "                (enc2[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv3), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        upconv4 = self.upconv4(dec3)\n",
    "        crop_bot, crop_top = crop_size(enc1, upconv4)\n",
    "        dec4 = self.decoder4(\n",
    "            torch.cat(\n",
    "                (enc1[:, :, crop_bot:crop_top, crop_bot:crop_top], upconv4), dim=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        output = self.out_conv(dec4)\n",
    "        return output\n",
    "\n",
    "\n",
    "def crop_size(encoder, upconv) -> tuple:\n",
    "    \"Return crop size of encoder's feature maps so it fits upconv's shape\"\n",
    "    x = encoder.shape[2]\n",
    "    y = upconv.shape[2]\n",
    "    return (x - y) // 2, (x + y) // 2\n",
    "\n",
    "\n",
    "def block(\n",
    "    in_channels: int, features: int, kernel_size: int, name: str\n",
    ") -> nn.Sequential:\n",
    "    # TODO add BatchNorm2d?\n",
    "    return nn.Sequential(\n",
    "        OrderedDict(\n",
    "            [\n",
    "                (f\"{name}_conv1\", nn.Conv2d(in_channels, features, kernel_size)),\n",
    "                (f\"{name}_relu1\", nn.ReLU(inplace=True)),\n",
    "                (f\"{name}_conv2\", nn.Conv2d(features, features, kernel_size)),\n",
    "                (f\"{name}_relu2\", nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA-Hf0OOBKPu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4551,
     "status": "ok",
     "timestamp": 1740324436219,
     "user": {
      "displayName": "Khan Ag",
      "userId": "07747718529479559494"
     },
     "user_tz": -60
    },
    "id": "8CVEzPJJBKPv",
    "outputId": "5fcf6ac2-a5a0-4426-ffa3-a2886ca6dbbb"
   },
   "outputs": [],
   "source": [
    "model.forward(torch.Tensor(1, 1, 572, 572)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-rILSc0Ihd0"
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir: str, mask_dir: str, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        # self.images = random.sample(os.listdir(image_dir), 1095)\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.image_data = []\n",
    "        self.mask_data = []\n",
    "        for image_name in self.images:\n",
    "            image_path = os.path.join(self.image_dir, image_name)\n",
    "            mask_path = os.path.join(self.mask_dir, image_name)\n",
    "            image = Image.open(image_path).convert(\"L\")\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "            self.image_data.append(image)\n",
    "            self.mask_data.append(mask)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        image = self.image_data[idx]\n",
    "        mask = self.mask_data[idx]\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class ImageMaskTransform:\n",
    "    def __init__(\n",
    "        self,\n",
    "        flip_prob=0.5,\n",
    "        rotate_prob=0.5,\n",
    "        # TODO adjust rotation degree, flip and rotation probabilities\n",
    "        rotation_degree=30,\n",
    "        train=True,\n",
    "        image_size=512,\n",
    "        input_size=572,\n",
    "        mask_size=388,\n",
    "        # TODO automatic rotate pad calc\n",
    "        rotate_pad=134,\n",
    "    ):\n",
    "        self.flip_prob = flip_prob\n",
    "        self.rotate_prob = rotate_prob\n",
    "        self.rotation_degree = rotation_degree\n",
    "        self.train = train\n",
    "        self.image_size = image_size\n",
    "        self.input_size = input_size\n",
    "        self.mask_size = mask_size\n",
    "        self.default_pad = (input_size - image_size) // 2\n",
    "        self.rotate_pad = rotate_pad\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        image = F.to_tensor(image)\n",
    "        mask = F.to_tensor(mask).long()\n",
    "        if self.train:\n",
    "            # TODO allow simultaneous augmentations\n",
    "            if random.random() < self.rotate_prob:\n",
    "                image = F.pad(image, padding=self.rotate_pad, padding_mode=\"reflect\")\n",
    "                angle = random.uniform(-self.rotation_degree, self.rotation_degree)\n",
    "                image = F.rotate(image, angle)\n",
    "                mask = F.rotate(mask, angle)\n",
    "                image = F.center_crop(image, self.input_size)\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "            elif random.random() < self.flip_prob:\n",
    "                image = F.hflip(image)\n",
    "                mask = F.hflip(mask)\n",
    "                image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "            elif random.random() < self.flip_prob:\n",
    "                image = F.vflip(image)\n",
    "                mask = F.vflip(mask)\n",
    "                image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "            else:\n",
    "                image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "                mask = F.center_crop(mask, self.mask_size)\n",
    "        else:\n",
    "            image = F.pad(image, padding=self.default_pad, padding_mode=\"reflect\")\n",
    "            mask = F.center_crop(mask, self.mask_size)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "width = 512\n",
    "angle_rad = math.radians(30)\n",
    "new_width = width * abs(math.cos(angle_rad)) + width * abs(math.sin(angle_rad))\n",
    "\n",
    "int(new_width), int(new_width - width) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpdCP4VJBKPx"
   },
   "outputs": [],
   "source": [
    "def iou_loss(predictions, targets, eps=1e-6):\n",
    "    intersection = torch.sum(predictions * targets)\n",
    "    union = torch.sum(predictions + targets) - intersection\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou\n",
    "\n",
    "\n",
    "class CrossEntropyIoULoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyIoULoss, self).__init__()\n",
    "        weight = torch.tensor([10., 1.])\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        ce_loss = self.ce(outputs, targets)\n",
    "        return ce_loss\n",
    "        # TODO add IOU\n",
    "        # return bce_loss + iou_loss(outputs, targets)\n",
    "        # TODO add border loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58933,
     "status": "ok",
     "timestamp": 1740324495154,
     "user": {
      "displayName": "Khan Ag",
      "userId": "07747718529479559494"
     },
     "user_tz": -60
    },
    "id": "QrXUBgZjBokq",
    "outputId": "e72c0d84-d669-45e1-bb0d-1501c3c28ff7"
   },
   "outputs": [],
   "source": [
    "local_machine_name = \"Dratini\"\n",
    "if socket.gethostname().endswith(local_machine_name):\n",
    "    base_dir = \"./\"\n",
    "else:\n",
    "    base_dir = \"/content/drive/MyDrive/Colab_Notebooks/Crack_Detection/\"\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2elhZbudEqD"
   },
   "outputs": [],
   "source": [
    "train_transform = ImageMaskTransform()\n",
    "\n",
    "train_image_dir = base_dir + \"isbi_2012_challenge/train/imgs\"\n",
    "train_mask_dir = base_dir + \"isbi_2012_challenge/train/labels\"\n",
    "batch_size = 1\n",
    "train_dataset = SegmentationDataset(\n",
    "    train_image_dir, train_mask_dir, transform=train_transform\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_image_dir = base_dir + \"isbi_2012_challenge/test/imgs\"\n",
    "test_mask_dir = base_dir + \"isbi_2012_challenge/test/labels\"\n",
    "\n",
    "test_transforms = ImageMaskTransform(train=False)\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_image_dir, test_mask_dir, transform=test_transforms\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1740324563403,
     "user": {
      "displayName": "Khan Ag",
      "userId": "07747718529479559494"
     },
     "user_tz": -60
    },
    "id": "KRcTftlvBKPz",
    "outputId": "fc1469d5-6d59-4766-982a-72df1e0a98eb"
   },
   "outputs": [],
   "source": [
    "for images, masks in train_dataloader:\n",
    "    image = images[0].permute(1, 2, 0).cpu().numpy()\n",
    "    mask = masks[0].cpu().numpy().squeeze()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axs[0].imshow(image, cmap=\"gray\")\n",
    "    axs[1].imshow(mask, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    print(image.shape[:2], mask.shape)\n",
    "    print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "executionInfo": {
     "elapsed": 24839,
     "status": "error",
     "timestamp": 1740324776412,
     "user": {
      "displayName": "Khan Ag",
      "userId": "07747718529479559494"
     },
     "user_tz": -60
    },
    "id": "mLDhgdX2BKPz",
    "outputId": "ea67f2a0-99ea-42c4-b497-a072ba61e8a6"
   },
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "num_epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = CrossEntropyIoULoss()\n",
    "\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / labels.numel()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.squeeze(1))\n",
    "            test_loss += loss.item()\n",
    "            test_accuracy += calculate_accuracy(outputs, labels.squeeze(1))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss / len(train_dataloader):.4f}, \"\n",
    "        f\"Test Loss: {test_loss / len(test_dataloader):.4f}, \"\n",
    "        f\"Test Accuracy: {test_accuracy / len(test_dataloader):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K52mxkXvBKP0"
   },
   "outputs": [],
   "source": [
    "data_iter = iter(test_dataloader)\n",
    "# data_iter = iter(train_dataloader)\n",
    "for _ in range(6):\n",
    "    images, masks = next(data_iter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "\n",
    "    image = images[0].permute(1, 2, 0).cpu().numpy()\n",
    "    mask = masks[0].cpu().numpy().squeeze()\n",
    "\n",
    "    predicted_mask = torch.argmax(outputs[0], dim=0).cpu().numpy()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    axs[0].imshow(image, cmap='gray')\n",
    "    axs[0].set_title(\"Image\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(mask, cmap=\"gray\")\n",
    "    axs[1].set_title(\"Ground Truth Mask\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    axs[2].imshow(predicted_mask, cmap=\"gray\")\n",
    "    axs[2].set_title(\"Predicted Mask\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbbzFJW_BKP1"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtaJDCsQBKP1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
